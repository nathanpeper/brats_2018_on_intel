{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69098c66-d83a-4966-abd8-73fc3c6beb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $CONDA_PREFIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e4d98a-9ee7-4715-a409-9bb605510e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras as K\n",
    "\n",
    "from habana_frameworks.tensorflow import load_habana_module\n",
    "\n",
    "load_habana_module()\n",
    "\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda32ed8-dad2-4543-8c3a-c4169cae9a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mkl_enabled_flag():\n",
    "\n",
    "    mkl_enabled = False\n",
    "    major_version = int(tf.__version__.split(\".\")[0])\n",
    "    minor_version = int(tf.__version__.split(\".\")[1])\n",
    "    if major_version >= 2:\n",
    "        if minor_version < 5:\n",
    "            from tensorflow.python import _pywrap_util_port\n",
    "        elif minor_version >= 9:\n",
    "\n",
    "            from tensorflow.python.util import _pywrap_util_port\n",
    "            onednn_enabled = int(os.environ.get('TF_ENABLE_ONEDNN_OPTS', '1'))\n",
    "\n",
    "        else:\n",
    "            from tensorflow.python.util import _pywrap_util_port\n",
    "            onednn_enabled = int(os.environ.get('TF_ENABLE_ONEDNN_OPTS', '0'))\n",
    "        mkl_enabled = _pywrap_util_port.IsMklEnabled() or (onednn_enabled == 1)\n",
    "    else:\n",
    "        mkl_enabled = tf.pywrap_tensorflow.IsMklEnabled()\n",
    "    return mkl_enabled\n",
    "\n",
    "print (\"We are using Tensorflow version\", tf.__version__)\n",
    "print(\"MKL enabled :\", get_mkl_enabled_flag())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55885ea3-3d8e-4191-b25f-d4eee684ab6f",
   "metadata": {},
   "source": [
    "## Define the settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e1c592-89d0-43f3-9b2d-ea6110e084c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33166fd1-098c-4094-9aac-5a3edc94f33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/home/ubuntu/unet/data/Task01_BrainTumour/\"\n",
    "\n",
    "TRAIN_TEST_SPLIT = 0.80\n",
    "VALIDATE_TEST_SPLIT = 0.50\n",
    "\n",
    "BATCH_SIZE_TRAIN = 8\n",
    "BATCH_SIZE_VALIDATE = 4\n",
    "BATCH_SIZE_TEST = 1\n",
    "\n",
    "TILE_HEIGHT = 144\n",
    "TILE_WIDTH = 144\n",
    "TILE_DEPTH = 144\n",
    "NUMBER_INPUT_CHANNELS = 1\n",
    "\n",
    "#CROP_DIM = (128,128,128,1)\n",
    "CROP_DIM = (144,144,144,1)\n",
    "\n",
    "NUMBER_OUTPUT_CLASSES = 1\n",
    "\n",
    "\n",
    "MODEL_DIR = Path(\"/home/ubuntu/unet/3D/models\")\n",
    "\n",
    "SAVED_MODEL_NAME = \"3d_unet_decathlon\"\n",
    "FILTERS = 16\n",
    "NUM_EPOCHS=40\n",
    "\n",
    "RANDOM_SEED = 64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9fd20d-cdb8-458d-8b96-22d79bfd39ce",
   "metadata": {},
   "source": [
    "## Define a data loader\n",
    "\n",
    "We'll use `tf.data` to define a way to load the BraTS dataset at runtime whenever a new batch of 3D images and masks are requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a446143-be3b-4c08-87b7-dc2dbc5921ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import DatasetGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83f55f8-de92-417c-87fd-184a7712f4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "brats_datafiles = DatasetGenerator(data_path=DATA_PATH, \n",
    "                                   train_test_split=TRAIN_TEST_SPLIT,\n",
    "                                   validate_test_split=VALIDATE_TEST_SPLIT,\n",
    "                                   batch_size_train=BATCH_SIZE_TRAIN,\n",
    "                                   batch_size_validate=BATCH_SIZE_VALIDATE,\n",
    "                                   batch_size_test=BATCH_SIZE_TEST,\n",
    "                                   tile_height=TILE_HEIGHT, \n",
    "                                   tile_width=TILE_WIDTH, \n",
    "                                   tile_depth=TILE_DEPTH, \n",
    "                                   number_input_channels=NUMBER_INPUT_CHANNELS,\n",
    "                                   number_output_classes=NUMBER_OUTPUT_CLASSES,\n",
    "                                   random_seed=RANDOM_SEED)\n",
    "brats_datafiles.print_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719360d8-448d-4503-afb6-c835a54c7928",
   "metadata": {},
   "source": [
    "## Plot some data samples\n",
    "\n",
    "Plots the MRI and Tumor Masks from a few data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d373f9f-f132-43a1-8efc-7d22e4dbeed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "brats_datafiles.display_train_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e44d4c2-90a3-445d-9c12-00689a05e1a3",
   "metadata": {},
   "source": [
    "## Define the loss and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3412f093-6a13-40e2-afb0-b6d9d0c06151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(target, prediction, axis=(1, 2, 3), smooth=0.0001):\n",
    "    \"\"\"\n",
    "    Sorenson Dice\n",
    "    \\frac{  2 \\times \\left | T \\right | \\cap \\left | P \\right |}{ \\left | T \\right | +  \\left | P \\right |  }\n",
    "    where T is ground truth mask and P is the prediction mask\n",
    "    \"\"\"\n",
    "    prediction = tf.round(prediction)  # Round to 0 or 1\n",
    "\n",
    "    intersection = tf.reduce_sum(target * prediction, axis=axis)\n",
    "    union = tf.reduce_sum(target + prediction, axis=axis)\n",
    "    numerator = tf.constant(2.) * intersection + smooth\n",
    "    denominator = union + smooth\n",
    "    coef = numerator / denominator\n",
    "\n",
    "    return tf.reduce_mean(coef)\n",
    "\n",
    "\n",
    "def soft_dice_coef(target, prediction, axis=(1, 2, 3), smooth=0.0001):\n",
    "    \"\"\"\n",
    "    Sorenson (Soft) Dice - Don't round predictions\n",
    "    \\frac{  2 \\times \\left | T \\right | \\cap \\left | P \\right |}{ \\left | T \\right | +  \\left | P \\right |  }\n",
    "    where T is ground truth mask and P is the prediction mask\n",
    "    \"\"\"\n",
    "    intersection = tf.reduce_sum(target * prediction, axis=axis)\n",
    "    union = tf.reduce_sum(target + prediction, axis=axis)\n",
    "    numerator = tf.constant(2.) * intersection + smooth\n",
    "    denominator = union + smooth\n",
    "    coef = numerator / denominator\n",
    "\n",
    "    return tf.reduce_mean(coef)\n",
    "\n",
    "\n",
    "def dice_loss(target, prediction, axis=(1, 2, 3), smooth=0.0001):\n",
    "    \"\"\"\n",
    "    Sorenson (Soft) Dice loss\n",
    "    Using -log(Dice) as the loss since it is better behaved.\n",
    "    Also, the log allows avoidance of the division which\n",
    "    can help prevent underflow when the numbers are very small.\n",
    "    \"\"\"\n",
    "    intersection = tf.reduce_sum(prediction * target, axis=axis)\n",
    "    p = tf.reduce_sum(prediction, axis=axis)\n",
    "    t = tf.reduce_sum(target, axis=axis)\n",
    "    numerator = tf.reduce_mean(intersection + smooth)\n",
    "    denominator = tf.reduce_mean(t + p + smooth)\n",
    "    dice_loss = -tf.math.log(2.*numerator) + tf.math.log(denominator)\n",
    "\n",
    "    return dice_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55b430e-27d9-4f3a-a69d-fb27e1754f55",
   "metadata": {},
   "source": [
    "## Define the 3D U-Net\n",
    "\n",
    "Create a TensorFlow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04c1a5d-c7c3-48a6-af3e-143bba164717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_3d(input_dim=CROP_DIM, \n",
    "            filters=FILTERS, \n",
    "            number_output_classes = NUMBER_OUTPUT_CLASSES,\n",
    "            use_upsampling=False, \n",
    "            concat_axis=-1,\n",
    "            model_name = SAVED_MODEL_NAME):\n",
    "    \"\"\"\n",
    "    3D U-Net\n",
    "    \"\"\"\n",
    "    \n",
    "    def ConvolutionBlock(x, name, filters, params):\n",
    "        \"\"\"\n",
    "        Convolutional block of layers\n",
    "        Per the original paper this is back to back 3D convs\n",
    "        with batch norm and then ReLU.\n",
    "        \"\"\"\n",
    "\n",
    "        x = K.layers.Conv3D(filters=filters, **params, name=name+\"_conv_0\")(x)\n",
    "        x = K.layers.BatchNormalization(name=name+\"_bn_0\")(x)\n",
    "        x = K.layers.Activation(\"relu\", name=name+\"_relu_0\")(x)\n",
    "\n",
    "        x = K.layers.Conv3D(filters=filters, **params, name=name+\"_conv_1\")(x)\n",
    "        x = K.layers.BatchNormalization(name=name+\"_bn_1\")(x)\n",
    "        x = K.layers.Activation(\"relu\", name=name)(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    # Convolution parameters\n",
    "    params = dict(kernel_size=(3, 3, 3), activation=None,\n",
    "                  padding=\"same\", \n",
    "                  kernel_initializer=\"he_uniform\")\n",
    "\n",
    "    # Transposed convolution parameters\n",
    "    params_trans = dict(kernel_size=(2, 2, 2), strides=(2, 2, 2),\n",
    "                        padding=\"same\",\n",
    "                        kernel_initializer=\"he_uniform\")\n",
    "\n",
    "    \n",
    "    inputs = K.layers.Input(shape=input_dim, name=\"mrimages\")\n",
    "\n",
    "    # BEGIN - Encoding path\n",
    "    encode_a = ConvolutionBlock(inputs, \"encode_a\", filters, params)\n",
    "    pool_a = K.layers.MaxPooling3D(name=\"pool_a\", pool_size=(2, 2, 2))(encode_a)\n",
    "\n",
    "    encode_b = ConvolutionBlock(pool_a, \"encode_b\", filters*2, params)\n",
    "    pool_b = K.layers.MaxPooling3D(name=\"pool_b\", pool_size=(2, 2, 2))(encode_b)\n",
    "\n",
    "    encode_c = ConvolutionBlock(pool_b, \"encode_c\", filters*4, params)\n",
    "    pool_c = K.layers.MaxPooling3D(name=\"pool_c\", pool_size=(2, 2, 2))(encode_c)\n",
    "\n",
    "    encode_d = ConvolutionBlock(pool_c, \"encode_d\", filters*8, params)\n",
    "    pool_d = K.layers.MaxPooling3D(name=\"pool_d\", pool_size=(2, 2, 2))(encode_d)\n",
    "\n",
    "    encode_e = ConvolutionBlock(pool_d, \"encode_e\", filters*16, params)\n",
    "    # END - Encoding path\n",
    "\n",
    "    \n",
    "    # BEGIN - Decoding path\n",
    "    if use_upsampling:\n",
    "        up = K.layers.UpSampling3D(name=\"up_e\", size=(2, 2, 2),\n",
    "                                   interpolation=\"bilinear\")(encode_e)\n",
    "    else:\n",
    "        up = K.layers.Conv3DTranspose(name=\"transconv_e\", filters=filters*8,\n",
    "                                      **params_trans)(encode_e)\n",
    "    concat_d = K.layers.concatenate(\n",
    "        [up, encode_d], axis=concat_axis, name=\"concat_d\")\n",
    "\n",
    "    decode_c = ConvolutionBlock(concat_d, \"decode_c\", filters*8, params)\n",
    "\n",
    "    if use_upsampling:\n",
    "        up = K.layers.UpSampling3D(name=\"up_c\", size=(2, 2, 2),\n",
    "                                   interpolation=\"bilinear\")(decode_c)\n",
    "    else:\n",
    "        up = K.layers.Conv3DTranspose(name=\"transconv_c\", filters=filters*4,\n",
    "                                      **params_trans)(decode_c)\n",
    "    concat_c = K.layers.concatenate(\n",
    "        [up, encode_c], axis=concat_axis, name=\"concat_c\")\n",
    "\n",
    "    decode_b = ConvolutionBlock(concat_c, \"decode_b\", filters*4, params)\n",
    "\n",
    "    if use_upsampling:\n",
    "        up = K.layers.UpSampling3D(name=\"up_b\", size=(2, 2, 2),\n",
    "                                   interpolation=\"bilinear\")(decode_b)\n",
    "    else:\n",
    "        up = K.layers.Conv3DTranspose(name=\"transconv_b\", filters=filters*2,\n",
    "                                      **params_trans)(decode_b)\n",
    "    concat_b = K.layers.concatenate(\n",
    "        [up, encode_b], axis=concat_axis, name=\"concat_b\")\n",
    "\n",
    "    decode_a = ConvolutionBlock(concat_b, \"decode_a\", filters*2, params)\n",
    "\n",
    "    if use_upsampling:\n",
    "        up = K.layers.UpSampling3D(name=\"up_a\", size=(2, 2, 2),\n",
    "                                   interpolation=\"bilinear\")(decode_a)\n",
    "    else:\n",
    "        up = K.layers.Conv3DTranspose(name=\"transconv_a\", filters=filters,\n",
    "                                      **params_trans)(decode_a)\n",
    "    concat_a = K.layers.concatenate(\n",
    "        [up, encode_a], axis=concat_axis, name=\"concat_a\")\n",
    "    \n",
    "    conv_out = ConvolutionBlock(concat_a, \"conv_out\", filters, params)\n",
    "\n",
    "    # END - Decoding path    \n",
    "\n",
    "    \n",
    "    prediction = K.layers.Conv3D(name=\"prediction_mask\",\n",
    "                                 filters=number_output_classes, kernel_size=(1, 1, 1),\n",
    "                                 activation=\"sigmoid\")(conv_out)\n",
    "\n",
    "    model = K.models.Model(inputs=[inputs], outputs=[prediction], name=model_name)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5689f3bd-35e0-47f0-a3c0-35ec3d2af158",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet_3d(input_dim=CROP_DIM, \n",
    "            filters=FILTERS, \n",
    "            number_output_classes = NUMBER_OUTPUT_CLASSES,\n",
    "            use_upsampling=False, \n",
    "            concat_axis=-1,\n",
    "            model_name = SAVED_MODEL_NAME)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=dice_loss, metrics=[dice_coef, soft_dice_coef])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79df6499-7ac8-4667-a87f-cb4af8093da7",
   "metadata": {},
   "source": [
    "## Define the training callbacks\n",
    "\n",
    "This includes model checkpoints and TensorBoard logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afac350-929e-4c64-a853-d38d9ca68aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_w_epoch = SAVED_MODEL_NAME + '_epoch_{epoch:02d}'\n",
    "saved_model_w_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e5b4ab-3427-442c-a65b-b6c2a9c60e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(MODEL_DIR / saved_model_w_epoch)\n",
    "checkpoint = K.callbacks.ModelCheckpoint(filepath=model_path,\n",
    "                                         verbose=1,\n",
    "                                         save_best_only=True)\n",
    "\n",
    "# TensorBoard\n",
    "logs_dir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tb_logs = K.callbacks.TensorBoard(log_dir=logs_dir)\n",
    "\n",
    "callbacks = [checkpoint, tb_logs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c6a9b2-f2a9-4248-a786-c6903298c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974b6211-1ac5-4b6c-a845-5f7be6df9c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13681371-3aa3-4435-b266-41c4f8ba8f75",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd2e90f-da6c-4867-b478-5a4ef240587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = brats_datafiles.num_train // BATCH_SIZE_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa439b2d-8491-44a5-b08e-0d79eadbe144",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(brats_datafiles.get_train(), \n",
    "          epochs=NUM_EPOCHS, \n",
    "          steps_per_epoch=steps_per_epoch,\n",
    "          validation_data=brats_datafiles.get_validate(), \n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e2b5b8-ba32-4a22-a310-9788ba4d2577",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "\n",
    "Evaluate the final model on the test dataset. This gives us an idea of how the model should perform on data it has never seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4abaca0-f97c-4f61-ac87-2ffc49e1ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_dice_coef, test_soft_dice_coef = model.evaluate(brats_datafiles.get_test())\n",
    "\n",
    "print(\"Average Dice Coefficient on test dataset = {:.4f}\".format(test_dice_coef))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b402065b-781c-4d8c-a1cd-26e08200ff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = Path(MODEL_DIR / SAVED_MODEL_NAME / SAVED_MODEL_NAME).with_suffix(\".h5\")\n",
    "model.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd41e0d-1a87-46d3-bfcd-51f7412a5473",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Absolute path where the base tf model in fp32 precisions is saved:\\n {save_path.resolve()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
